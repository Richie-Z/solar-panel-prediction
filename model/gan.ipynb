{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN MODEL TRAINING\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import find_missing_date_ranges\n",
    "from enums import (\n",
    "    DatasetColumns,\n",
    "    WeatherDatasetColumns\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "\n",
    "# Ensure all operations run on GPU when possible\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "FILE_NAME = \"dataset.csv\"\n",
    "WEATHER_DATASET = \"dataset_weather.csv\"\n",
    "\n",
    "original_data = pd.read_csv(\n",
    "    FILE_NAME,\n",
    "    parse_dates=[DatasetColumns.STATISTICAL_PERIOD.value],\n",
    "    index_col=DatasetColumns.STATISTICAL_PERIOD.value,\n",
    ")\n",
    "\n",
    "weather_data = pd.read_csv(\n",
    "    WEATHER_DATASET,\n",
    "    parse_dates=[WeatherDatasetColumns.DATETIME.value],\n",
    "    index_col=WeatherDatasetColumns.DATETIME.value,\n",
    ").asfreq(\"h\")\n",
    "\n",
    "weather_features = [\n",
    "    WeatherDatasetColumns.TEMPERATURE_C.value,\n",
    "    WeatherDatasetColumns.HUMIDITY_PERCENT.value,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing date ranges\n",
    "gap_start, gap_end = find_missing_date_ranges(\n",
    "    original_data, DatasetColumns.STATISTICAL_PERIOD.value\n",
    ")\n",
    "gap_dates = pd.date_range(start=gap_start, end=gap_end, freq=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "pre_gap_data = original_data[original_data.index < gap_start].asfreq(\"h\")\n",
    "post_gap_data = original_data[original_data.index >= gap_end].asfreq(\"h\")\n",
    "\n",
    "pre_gap_train_size = int(len(pre_gap_data) * 0.8)\n",
    "pre_gap_train = pre_gap_data.iloc[:pre_gap_train_size].copy()\n",
    "pre_gap_test = pre_gap_data.iloc[pre_gap_train_size:]\n",
    "\n",
    "pre_gap_train.loc[:, DatasetColumns.PV_YIELD.value] = pre_gap_train[\n",
    "    DatasetColumns.PV_YIELD.value\n",
    "].interpolate(method=\"linear\")\n",
    "\n",
    "\n",
    "pre_weather_data = weather_data[weather_data.index < gap_start].bfill()\n",
    "pre_weather_data = pre_weather_data.reindex(pre_gap_data.index)\n",
    "pre_weather_data_test = pre_weather_data.reindex(pre_gap_test.index)\n",
    "\n",
    "\n",
    "gap_weather_data = weather_data.reindex(gap_dates).ffill()\n",
    "post_weather_data = weather_data[weather_data.index >= gap_end].bfill()\n",
    "\n",
    "pre_gap_train_combined = pre_gap_train.join(\n",
    "    pre_weather_data[weather_features], how=\"inner\"\n",
    ")\n",
    "pre_gap_test_combined = pre_gap_test.join(\n",
    "    pre_weather_data_test[weather_features], how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Hyperparameters\n",
    "LATENT_DIM = 128\n",
    "LEARNING_RATE_G = 2e-5  \n",
    "LEARNING_RATE_D = 1e-5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN-GP Architecure\n",
    "def build_generator(latent_dim, num_features):\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        noise_input = tf.keras.Input(shape=(latent_dim,))\n",
    "        weather_input = tf.keras.Input(shape=(num_features,))\n",
    "\n",
    "        # Rest of the generator code remains the same\n",
    "        weather_x = tf.keras.layers.BatchNormalization()(weather_input)\n",
    "        weather_x = tf.keras.layers.Dense(32)(weather_x)\n",
    "        weather_x = tf.keras.layers.LeakyReLU(alpha=0.2)(weather_x)\n",
    "        weather_x = tf.keras.layers.BatchNormalization()(weather_x)\n",
    "\n",
    "        noise_x = tf.keras.layers.Dense(64)(noise_input)\n",
    "        noise_x = tf.keras.layers.LeakyReLU(alpha=0.2)(noise_x)\n",
    "        noise_x = tf.keras.layers.BatchNormalization()(noise_x)\n",
    "\n",
    "        x = tf.keras.layers.Concatenate()([noise_x, weather_x])\n",
    "\n",
    "        def dense_block(x, units, dropout_rate=0.3):\n",
    "            skip = x\n",
    "            skip = (\n",
    "                tf.keras.layers.Dense(units)(skip) if skip.shape[-1] != units else skip\n",
    "            )\n",
    "\n",
    "            x = tf.keras.layers.Dense(units)(x)\n",
    "            x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "            x = tf.keras.layers.Dense(units)(x)\n",
    "            x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "            return tf.keras.layers.Add()([x, skip])\n",
    "\n",
    "        x = dense_block(x, 512, 0.3)\n",
    "        x = dense_block(x, 256, 0.3)\n",
    "        x = dense_block(x, 256, 0.3)\n",
    "        x = dense_block(x, 128, 0.3)\n",
    "        x = dense_block(x, 128, 0.3)\n",
    "\n",
    "        x = tf.keras.layers.Dense(64)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "        output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        return tf.keras.Model(inputs=[noise_input, weather_input], outputs=output)\n",
    "\n",
    "\n",
    "def build_discriminator(num_features):\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        # Rest of discriminator code remains the same\n",
    "        pv_input = tf.keras.Input(shape=(1,))\n",
    "        weather_input = tf.keras.Input(shape=(num_features,))\n",
    "\n",
    "        pv_normalized = tf.keras.layers.BatchNormalization()(pv_input)\n",
    "        weather_normalized = tf.keras.layers.BatchNormalization()(weather_input)\n",
    "\n",
    "        x = tf.keras.layers.Concatenate()([pv_normalized, weather_normalized])\n",
    "\n",
    "        def critic_block(x, units, dropout_rate=0.3):\n",
    "            x = tf.keras.layers.Dense(units)(x)\n",
    "            x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            return tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        x = critic_block(x, 128, 0.3)\n",
    "        x = critic_block(x, 256, 0.3)\n",
    "        x = critic_block(x, 512, 0.3)\n",
    "        x = critic_block(x, 256, 0.3)\n",
    "\n",
    "        output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "        return tf.keras.Model(\n",
    "            inputs=[pv_input, weather_input], outputs=output, name=\"Discriminator\"\n",
    "        )\n",
    "\n",
    "\n",
    "class SolarGAN(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, num_features):\n",
    "        super(SolarGAN, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            self.generator = build_generator(latent_dim, num_features)\n",
    "            self.discriminator = build_discriminator(num_features)\n",
    "        self.gp_weight = tf.cast(10.0, tf.float32)\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer):\n",
    "        super(SolarGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        real_pv, weather_features = data\n",
    "        batch_size = tf.shape(real_pv)[0]\n",
    "        d_steps = 3\n",
    "        g_steps = 1\n",
    "\n",
    "        d_loss_avg = tf.cast(0.0, tf.float32)\n",
    "        for _ in range(d_steps):\n",
    "            noise = tf.random.normal([batch_size, self.latent_dim], dtype=tf.float32)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_pv = self.generator([noise, weather_features], training=True)\n",
    "                fake_pv = tf.cast(fake_pv, tf.float32)\n",
    "\n",
    "                real_pred = self.discriminator(\n",
    "                    [real_pv, weather_features], training=True\n",
    "                )\n",
    "                fake_pred = self.discriminator(\n",
    "                    [fake_pv, weather_features], training=True\n",
    "                )\n",
    "\n",
    "                alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0, dtype=tf.float32)\n",
    "                interpolated = real_pv + alpha * (fake_pv - real_pv)\n",
    "\n",
    "                with tf.GradientTape() as gp_tape:\n",
    "                    gp_tape.watch(interpolated)\n",
    "                    interp_pred = self.discriminator(\n",
    "                        [interpolated, weather_features], training=True\n",
    "                    )\n",
    "\n",
    "                grads = gp_tape.gradient(interp_pred, interpolated)\n",
    "                grad_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "                gradient_penalty = tf.reduce_mean(tf.square(grad_norm - 1.0))\n",
    "\n",
    "                d_loss = (\n",
    "                    tf.reduce_mean(fake_pred)\n",
    "                    - tf.reduce_mean(real_pred)\n",
    "                    + self.gp_weight * gradient_penalty\n",
    "                )\n",
    "\n",
    "            d_gradients = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradients, self.discriminator.trainable_variables)\n",
    "            )\n",
    "            d_loss_avg += d_loss\n",
    "\n",
    "        g_loss_avg = tf.cast(0.0, tf.float32)\n",
    "        for _ in range(g_steps):\n",
    "            noise = tf.random.normal([batch_size, self.latent_dim], dtype=tf.float32)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_pv = self.generator([noise, weather_features], training=True)\n",
    "                fake_pv = tf.cast(fake_pv, tf.float32)\n",
    "                fake_pred = self.discriminator(\n",
    "                    [fake_pv, weather_features], training=True\n",
    "                )\n",
    "\n",
    "                wasserstein_loss = -tf.reduce_mean(fake_pred)\n",
    "                l1_loss = 0.2 * tf.reduce_mean(tf.abs(fake_pv - real_pv))\n",
    "                l2_loss = 0.1 * tf.reduce_mean(tf.square(fake_pv - real_pv))\n",
    "                smoothness_loss = 0.1 * tf.reduce_mean(\n",
    "                    tf.abs(fake_pv[1:] - fake_pv[:-1])\n",
    "                )\n",
    "\n",
    "                g_loss = wasserstein_loss + l1_loss + l2_loss + smoothness_loss\n",
    "\n",
    "            g_gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(g_gradients, self.generator.trainable_variables)\n",
    "            )\n",
    "            g_loss_avg += g_loss\n",
    "\n",
    "        return {\n",
    "            \"d_loss\": d_loss_avg / tf.cast(d_steps, tf.float32),\n",
    "            \"g_loss\": g_loss_avg / tf.cast(g_steps, tf.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "def prepare_data(pre_gap_train_combined, pre_gap_test_combined, weather_features):\n",
    "    combined_columns = [DatasetColumns.PV_YIELD.value] + weather_features\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    with tf.device('/GPU:0'):\n",
    "        train_scaled = scaler.fit_transform(pre_gap_train_combined[combined_columns])\n",
    "        train_pv = tf.cast(train_scaled[:, 0:1], tf.float32)\n",
    "        train_weather = tf.cast(train_scaled[:, 1:], tf.float32)\n",
    "\n",
    "        test_scaled = scaler.transform(pre_gap_test_combined[combined_columns])\n",
    "        test_pv = tf.cast(test_scaled[:, 0:1], tf.float32)\n",
    "        test_weather = tf.cast(test_scaled[:, 1:], tf.float32)\n",
    "\n",
    "        # Use tf.data.Dataset with prefetch for better GPU utilization\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_pv, train_weather))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((test_pv, test_weather))\n",
    "        test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SOLAR GAN\n",
    "def train_solar_gan(train_dataset, num_features):\n",
    "\n",
    "    with tf.device('/GPU:0'):\n",
    "      solar_gan = SolarGAN(LATENT_DIM, num_features)\n",
    "      solar_gan.compile(\n",
    "          g_optimizer=tf.keras.optimizers.Adam(\n",
    "              learning_rate=LEARNING_RATE_G, beta_1=0.5, beta_2=0.9\n",
    "          ),\n",
    "          d_optimizer=tf.keras.optimizers.Adam(\n",
    "              learning_rate=LEARNING_RATE_D, beta_1=0.5, beta_2=0.9\n",
    "          ),\n",
    "      )\n",
    "\n",
    "      history = {\"d_loss\": [], \"g_loss\": []}\n",
    "      best_loss = float(\"inf\")\n",
    "      patience = 100\n",
    "      patience_counter = 0\n",
    "      min_epochs = 300\n",
    "\n",
    "      initial_lr_g = 2e-5\n",
    "      initial_lr_d = 1e-5\n",
    "\n",
    "      for epoch in range(EPOCHS):\n",
    "\n",
    "          if epoch > 0 and epoch % 200 == 0:\n",
    "              solar_gan.g_optimizer.learning_rate = LEARNING_RATE_G * 0.9\n",
    "              solar_gan.d_optimizer.learning_rate = LEARNING_RATE_D * 0.9\n",
    "              initial_lr_g *= 0.9\n",
    "              initial_lr_d *= 0.9\n",
    "\n",
    "          d_losses = []\n",
    "          g_losses = []\n",
    "\n",
    "          for batch_data in train_dataset:\n",
    "              losses = solar_gan.train_step(batch_data)\n",
    "              d_losses.append(float(losses[\"d_loss\"]))\n",
    "              g_losses.append(float(losses[\"g_loss\"]))\n",
    "\n",
    "          avg_d_loss = np.mean(d_losses)\n",
    "          avg_g_loss = np.mean(g_losses)\n",
    "          history[\"d_loss\"].append(avg_d_loss)\n",
    "          history[\"g_loss\"].append(avg_g_loss)\n",
    "\n",
    "          print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "          print(f\"D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "          if epoch >= min_epochs:\n",
    "              current_loss = abs(avg_d_loss) + abs(avg_g_loss)\n",
    "              if current_loss < best_loss * 0.999:\n",
    "                  best_loss = current_loss\n",
    "                  patience_counter = 0\n",
    "              else:\n",
    "                  patience_counter += 1\n",
    "\n",
    "              if patience_counter >= patience:\n",
    "                  print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                  break\n",
    "\n",
    "      return solar_gan, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history[\"d_loss\"], label=\"Discriminator Loss\")\n",
    "    plt.plot(history[\"g_loss\"], label=\"Generator Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training History\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def generate_predictions(model, weather_features, scaler):\n",
    "    weather_features = tf.cast(weather_features, tf.float32)\n",
    "    batch_size = tf.shape(weather_features)[0]\n",
    "\n",
    "    noise = tf.random.normal([batch_size, LATENT_DIM], dtype=tf.float32)    \n",
    "    predictions_scaled = model.generator([noise, weather_features], training=False)\n",
    "\n",
    "    predictions_with_weather = np.concatenate(\n",
    "        [predictions_scaled.numpy(), weather_features.numpy()], axis=1\n",
    "    )\n",
    "    predictions = scaler.inverse_transform(predictions_with_weather)[:, 0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(model, test_dataset, scaler):\n",
    "    all_predictions = []\n",
    "    all_true_values = []\n",
    "\n",
    "    for test_pv, test_weather in test_dataset:\n",
    "        batch_predictions = generate_predictions(model, test_weather, scaler)\n",
    "        all_predictions.extend(batch_predictions)\n",
    "        all_true_values.extend(test_pv.numpy().flatten())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_true_values = np.array(all_true_values)\n",
    "    \n",
    "    mse = np.mean((all_predictions - all_true_values) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_true_values))\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "    return all_predictions, all_true_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pre_gap_data` training model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 22:41:39.868247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-02-11 22:41:39.868474: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-02-11 22:41:39.868497: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-02-11 22:41:39.868734: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-11 22:41:39.868812: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Prepare PRE-GAP train and test data\n",
    "train_dataset, test_dataset, scaler = prepare_data(\n",
    "    pre_gap_train_combined,\n",
    "    pre_gap_test_combined,\n",
    "    weather_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richie/miniforge3/envs/p39/lib/python3.9/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "2025-02-11 22:41:47.658742: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_weather_features = len(weather_features)\n",
    "solar_gan, history = train_solar_gan(train_dataset, num_weather_features)\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on test set\n",
    "predictions, true_values = evaluate_model(solar_gan, test_dataset, scaler)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(true_values, label='True Values', color='orange')\n",
    "plt.plot(predictions, label='Predicted Values',color='green')\n",
    "plt.legend()\n",
    "plt.title('WGAP-GP Predictions vs Actual')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
