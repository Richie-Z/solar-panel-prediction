{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN MODEL TRAINING\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import (\n",
    "    find_missing_date_ranges,\n",
    ")\n",
    "\n",
    "from gan import (build_generator, build_discriminator, SolarGAN)\n",
    "\n",
    "from enums import (\n",
    "    DatasetColumns,\n",
    "    WeatherDatasetColumns\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "FILE_NAME = \"dataset.csv\"\n",
    "WEATHER_DATASET = \"dataset_weather.csv\"\n",
    "\n",
    "\n",
    "original_data = pd.read_csv(\n",
    "    FILE_NAME,\n",
    "    parse_dates=[DatasetColumns.STATISTICAL_PERIOD.value],\n",
    "    index_col=DatasetColumns.STATISTICAL_PERIOD.value,\n",
    ")\n",
    "\n",
    "weather_data = pd.read_csv(\n",
    "    WEATHER_DATASET,\n",
    "    parse_dates=[WeatherDatasetColumns.DATETIME.value],\n",
    "    index_col=WeatherDatasetColumns.DATETIME.value,\n",
    ").asfreq(\"h\")\n",
    "\n",
    "weather_features = [\n",
    "    WeatherDatasetColumns.TEMPERATURE_C.value,\n",
    "    WeatherDatasetColumns.HUMIDITY_PERCENT.value,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing date ranges\n",
    "gap_start, gap_end = find_missing_date_ranges(\n",
    "    original_data, DatasetColumns.STATISTICAL_PERIOD.value\n",
    ")\n",
    "gap_dates = pd.date_range(start=gap_start, end=gap_end, freq=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "pre_gap_data = original_data[original_data.index < gap_start].asfreq(\"h\")\n",
    "post_gap_data = original_data[original_data.index >= gap_end].asfreq(\"h\")\n",
    "\n",
    "pre_gap_train_size = int(len(pre_gap_data) * 0.8)\n",
    "pre_gap_train = pre_gap_data.iloc[:pre_gap_train_size].copy()\n",
    "pre_gap_test = pre_gap_data.iloc[pre_gap_train_size:]\n",
    "\n",
    "pre_gap_train.loc[:, DatasetColumns.PV_YIELD.value] = pre_gap_train[\n",
    "    DatasetColumns.PV_YIELD.value\n",
    "].interpolate(method=\"linear\")\n",
    "\n",
    "\n",
    "pre_weather_data = weather_data[weather_data.index < gap_start].bfill()\n",
    "pre_weather_data = pre_weather_data.reindex(pre_gap_data.index)\n",
    "pre_weather_data_test = pre_weather_data.reindex(pre_gap_test.index)\n",
    "\n",
    "\n",
    "gap_weather_data = weather_data.reindex(gap_dates).ffill()\n",
    "post_weather_data = weather_data[weather_data.index >= gap_end].bfill()\n",
    "\n",
    "\n",
    "pre_gap_train_combined = pre_gap_train.join(\n",
    "    pre_weather_data[weather_features], how=\"inner\"\n",
    ")\n",
    "pre_gap_test_combined = pre_gap_test.join(\n",
    "    pre_weather_data_test[weather_features], how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Hyperparameters\n",
    "LATENT_DIM = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(pre_gap_train_combined, pre_gap_test_combined, weather_features):\n",
    "    # Scale the data\n",
    "    combined_columns = [DatasetColumns.PV_YIELD.value] + weather_features\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit and transform training data\n",
    "    train_scaled = scaler.fit_transform(pre_gap_train_combined[combined_columns])\n",
    "    train_pv = train_scaled[:, 0:1].astype(np.float32)  # Convert to float32\n",
    "    train_weather = train_scaled[:, 1:].astype(np.float32)  # Convert to float32\n",
    "\n",
    "    # Transform test data\n",
    "    test_scaled = scaler.transform(pre_gap_test_combined[combined_columns])\n",
    "    test_pv = test_scaled[:, 0:1].astype(np.float32)\n",
    "    test_weather = test_scaled[:, 1:].astype(np.float32)\n",
    "\n",
    "    # Create TensorFlow datasets with float32 data\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(train_pv, tf.float32), tf.cast(train_weather, tf.float32))\n",
    "    )\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(test_pv, tf.float32), tf.cast(test_weather, tf.float32))\n",
    "    )\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return train_dataset, test_dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_solar_gan(train_dataset, num_features):\n",
    "    # Create model\n",
    "    solar_gan = SolarGAN(LATENT_DIM, num_features)\n",
    "\n",
    "    # Use slightly higher learning rate but still small enough for stability\n",
    "    lr = 1e-4\n",
    "    solar_gan.compile(\n",
    "        g_optimizer=tf.keras.optimizers.Adam(lr, beta_1=0.5, beta_2=0.9),\n",
    "        d_optimizer=tf.keras.optimizers.Adam(lr, beta_1=0.5, beta_2=0.9),\n",
    "    )\n",
    "\n",
    "    # Training history\n",
    "    history = {\"d_loss\": [], \"g_loss\": []}\n",
    "\n",
    "    # Modified early stopping parameters\n",
    "    best_loss = float(\"inf\")\n",
    "    patience = 50  # Increased patience\n",
    "    patience_counter = 0\n",
    "    min_epochs = 500  # Minimum number of epochs before early stopping\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_d_loss = []\n",
    "        epoch_g_loss = []\n",
    "\n",
    "        for batch_data in train_dataset:\n",
    "            losses = solar_gan.train_step(batch_data)\n",
    "            epoch_d_loss.append(float(losses[\"d_loss\"]))\n",
    "            epoch_g_loss.append(float(losses[\"g_loss\"]))\n",
    "\n",
    "        avg_d_loss = np.mean(epoch_d_loss)\n",
    "        avg_g_loss = np.mean(epoch_g_loss)\n",
    "\n",
    "        history[\"d_loss\"].append(avg_d_loss)\n",
    "        history[\"g_loss\"].append(avg_g_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{EPOCHS} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Early stopping with minimum epochs and modified loss calculation\n",
    "        if epoch >= min_epochs:\n",
    "            # Modified loss calculation to prevent premature stopping\n",
    "            current_loss = abs(avg_d_loss) + abs(avg_g_loss)\n",
    "            if current_loss < best_loss * 1.1:  # Allow 10% variation\n",
    "                best_loss = min(current_loss, best_loss)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    return solar_gan, history\n",
    "\n",
    "\n",
    "# Add a function to visualize training progress\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history[\"d_loss\"], label=\"Discriminator Loss\")\n",
    "    plt.plot(history[\"g_loss\"], label=\"Generator Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training History\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, weather_features, scaler):\n",
    "    # Ensure weather_features is float32\n",
    "    weather_features = tf.cast(weather_features, tf.float32)\n",
    "    batch_size = tf.shape(weather_features)[0]\n",
    "\n",
    "    # Create noise input\n",
    "    noise = tf.random.normal([batch_size, LATENT_DIM], dtype=tf.float32)\n",
    "\n",
    "    # Generate predictions by passing noise and weather features separately\n",
    "    predictions_scaled = model.generator([noise, weather_features], training=False)\n",
    "\n",
    "    # Convert predictions to numpy and prepare for inverse transform\n",
    "    predictions_with_weather = np.concatenate(\n",
    "        [predictions_scaled.numpy(), weather_features.numpy()], axis=1\n",
    "    )\n",
    "    predictions = scaler.inverse_transform(predictions_with_weather)[:, 0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Example usage in evaluation\n",
    "def evaluate_model(model, test_dataset, scaler):\n",
    "    all_predictions = []\n",
    "    all_true_values = []\n",
    "\n",
    "    for test_pv, test_weather in test_dataset:\n",
    "        # Generate predictions for this batch\n",
    "        batch_predictions = generate_predictions(model, test_weather, scaler)\n",
    "\n",
    "        # Store predictions and true values\n",
    "        all_predictions.extend(batch_predictions)\n",
    "        all_true_values.extend(test_pv.numpy().flatten())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_true_values = np.array(all_true_values)\n",
    "\n",
    "    # Calculate metrics (e.g., MSE, MAE)\n",
    "    mse = np.mean((all_predictions - all_true_values) ** 2)\n",
    "    mae = np.mean(np.abs(all_predictions - all_true_values))\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "    return all_predictions, all_true_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pre_gap_data` training model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, scaler = prepare_data(\n",
    "        pre_gap_train_combined,\n",
    "        pre_gap_test_combined,\n",
    "        weather_features\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richie/.venv/dev/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_weather_features = len(weather_features)\n",
    "solar_gan, history = train_solar_gan(train_dataset, num_weather_features)\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating model on test set...\")\n",
    "predictions, true_values = evaluate_model(solar_gan, test_dataset, scaler)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(true_values, label='True Values', marker='o')\n",
    "plt.plot(predictions, label='Predicted Values', marker='x')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('PV Yield')\n",
    "plt.legend()\n",
    "plt.title('True vs Predicted PV Yield (First 100 samples)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
